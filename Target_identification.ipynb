{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccc3ff04-65ee-47ff-80d1-d2d8e3c91afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cuml.accel extension is already loaded. To reload it, use:\n",
      "  %reload_ext cuml.accel\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from scipy.sparse.linalg import eigsh, eigs\n",
    "import torch\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "import torch_geometric\n",
    "from torch.amp import GradScaler, autocast\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import random_split\n",
    "from torch.nn import Linear, Parameter\n",
    "from torch_geometric.nn import MessagePassing,GraphSAGE,GlobalAttention\n",
    "from torch_geometric.utils import degree,add_self_loops\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn.pool import global_mean_pool\n",
    "from torch_geometric.data import HeteroData, DataLoader,Data, Batch\n",
    "from torch_geometric.loader import NeighborLoader, ImbalancedSampler, DataLoader\n",
    "import pandas as pd\n",
    "from torch_geometric.nn.pool import global_mean_pool\n",
    "from torch_geometric.nn import summary\n",
    "from scipy.stats import hypergeom\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "from torch_geometric.nn import Node2Vec\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from cuml.preprocessing import normalize, StandardScaler\n",
    "import tqdm\n",
    "from IPython.display import display, clear_output\n",
    "from sklearn.metrics import (accuracy_score,roc_auc_score,classification_report,precision_score,recall_score,\n",
    "f1_score,roc_curve,precision_recall_curve,confusion_matrix,average_precision_score,fbeta_score)\n",
    "%load_ext cuml.accel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc67f7f7-7fd0-4ed7-bbcf-1fdf00dbe59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.read_graphml('/home/bionik/AI_ML/FDA_Project/Data/GO_enrichment/GO_ontology.graphml')\n",
    "id2node={i:u for i,(u,v) in enumerate(G.nodes(data=True))}\n",
    "node2id={u:i for i,(u,v) in enumerate(G.nodes(data=True))}\n",
    "\n",
    "G_reindexed = nx.relabel_nodes(G, node2id)\n",
    "edges=[]\n",
    "for u, v ,data in G_reindexed.edges(data=True):\n",
    "    edges.append([u,v])\n",
    "edge_index=np.array(edges)\n",
    "edge_index=edge_index.transpose()\n",
    "edge_index=torch.tensor(edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f07f2f2-af87-4baf-9506-51867acd4e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "node2vec=Node2Vec(edge_index=edge_index,embedding_dim=64, sparse=True,\n",
    "                 walk_length=80,context_size=10,walks_per_node=8, p=0.85,q=1,num_negative_samples=3)\n",
    "loader=node2vec.loader(batch_size=128,shuffle=True)\n",
    "optimizer=torch.optim.SparseAdam(list(node2vec.parameters()),lr=0.01)\n",
    "node2vec=node2vec.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf6242-c35c-4fcc-b610-1ab41257ae5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=8\n",
    "losses=[]\n",
    "for i in range(1,epoch+1):\n",
    "    x=train_model_node2vec()\n",
    "    print(x)\n",
    "    losses.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc2bf0e-7e53-4081-836c-30e3f1614758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_node2vec():\n",
    "    total_loss_per_epoch=0\n",
    "    node2vec.train()\n",
    "    for pos,neg in loader:\n",
    "        optimizer.zero_grad()\n",
    "        pos=pos.to(device)\n",
    "        neg=neg.to(device)\n",
    "        loss=node2vec.loss(pos,neg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss_per_epoch+=loss.item()\n",
    "        del pos,neg\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    return total_loss_per_epoch/len(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdd723f-1bea-4ff0-adaa-b41c1343bd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "node2vec.load_state_dict(state_dict)\n",
    "Z=np.array(node2vec().detach().cpu())\n",
    "Z = Z.astype('float32')\n",
    "norm_Z=normalize(Z,norm='l2',axis=1)\n",
    "\n",
    "for i,(k,v) in enumerate(G_reindexed.nodes(data=True)):   # for all GO in togherte     for i,(k,v) in enumerate(id2node.items()):\n",
    "    value=norm_Z[i]\n",
    "    key=id2node[k]\n",
    "    embedding_GO[key]=value\n",
    "embedding_Go_id={node2id.get(i):v for i,v in embedding_GO.items()}\n",
    "embedding_Go_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5834699-bac7-41af-b742-c3ef9c8533cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_gene(annotation_of_gene, embedding_matrix,node2id,gene='NUDT4B', concat=False):\n",
    "    ## genegene2ass_all {'NUDT4B': {'GO:0000298',...} dict with value is set\n",
    "    ids=[] ## make sure the ids in node2id is not coming as None\n",
    "    ass=list(annotation_of_gene)\n",
    "    for i in ass:\n",
    "        k=node2id.get(i)\n",
    "        if k:\n",
    "            ids.append(k)\n",
    "    emb=np.sum(embedding_matrix[ids],axis=0)\n",
    "    if concat:\n",
    "        emb=features_x[ids].flatten()\n",
    "        return emb\n",
    "    return emb/emb.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad14545-4410-418b-a281-3f9d1762435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prot_emb={}\n",
    "for i,v in gene2ass_all_ncbi.items():\n",
    "    if v:\n",
    "        prot_emb[sys2gene[i]]=get_embeddings_gene(annotation_of_gene=v, embedding_matrix=embedding_GO,gene=i, concat=False,node2id=GO_embeddings_GO_2_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb87d91-887b-462d-99bd-d961a477c4c6",
   "metadata": {},
   "source": [
    "From this start the file running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e26b23bd-8258-4d1a-8811-5729e49cce46",
   "metadata": {},
   "outputs": [],
   "source": [
    "genes_in_pathway=defaultdict(set)\n",
    "\n",
    "list_of_all_graphml=[]\n",
    "for i in os.listdir('/home/bionik/AI_ML/FDA_Project/Data/Graphs'):\n",
    "    list_of_all_graphml.append(str(i).removesuffix('.graphml').lower())\n",
    "\n",
    "with open('/home/bionik/AI_ML/FDA_Project/Data/final_data/gmts/ReactomePathways.gmt','r') as f:\n",
    "    for lines in f:\n",
    "        key=lines.split('\\t')[0].split('_')[0].lower()\n",
    "        genes=lines.strip('\\n').split('\\t')[2:]\n",
    "        genes_in_pathway[key].update(genes)\n",
    "genes_in_pathway=dict(genes_in_pathway)\n",
    "\n",
    "embedding_GO=np.load('/home/bionik/AI_ML/FDA_Project/Data/New/GO_embeddings.npy')\n",
    "\n",
    "with open('/home/bionik/AI_ML/FDA_Project/Data/New/GO_embeddings_ids.pkl', 'rb') as file:\n",
    "    GO_embeddings_ids2node = pickle.load(file)\n",
    "\n",
    "GO_embeddings_GO_2_id={i:v for v,i in GO_embeddings_ids2node.items()}\n",
    "\n",
    "with open('/home/bionik/AI_ML/FDA_Project/Data/New/gene2ass_all_ncbi.pkl', 'rb') as file:\n",
    "    gene2ass_all_ncbi = pickle.load(file)\n",
    "\n",
    "with open('/home/bionik/AI_ML/FDA_Project/Data/New/synonmys_of_genes.pkl', 'rb') as file:\n",
    "    sys2gene = pickle.load(file)\n",
    "\n",
    "with open('/home/bionik/AI_ML/FDA_Project/Data/New/Protein_embeddings.pkl', 'rb') as file:\n",
    "    prot_emb = pickle.load(file)\n",
    "    \n",
    "with open('/home/bionik/AI_ML/FDA_Project/Data/New/disease_target_aff_genes.pkl', 'rb') as file:\n",
    "    disease_target_aff_genes = pickle.load(file)\n",
    "\n",
    "with open('/home/bionik/AI_ML/FDA_Project/Data/New/disease_involment_genes.pkl', 'rb') as file:\n",
    "    disease_involment_genes = pickle.load(file)\n",
    "\n",
    "with open('/home/bionik/AI_ML/FDA_Project/Data/New/all_targets_ori_and_CHEMBL.pkl', 'rb') as file:\n",
    "    disease2target = pickle.load(file)\n",
    "\n",
    "with open('/home/bionik/AI_ML/FDA_Project/Data/New/pathway_enrichment_graphs.pkl', 'rb') as file:\n",
    "    pathway_enrichment = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "bcb2ed05-2e53-490e-b12d-6d553fb8e993",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_pathway_union(lists_of_pathways,path='/home/bionik/AI_ML/FDA_Project/Data/Graphs'):\n",
    "    list_=[]\n",
    "    ultra_mapping={}\n",
    "    for idx,pathway in enumerate(lists_of_pathways):\n",
    "        pathway1=pathway.replace('/','_')\n",
    "        idx1=nx.read_graphml(f\"{path}/{pathway1}.graphml\")\n",
    "        list_.append(idx1)\n",
    "        mapping = {f\"{pathway}{node}\":node for node,data in idx1.nodes(data=True) if data.get('label')!='Small_molecule'}\n",
    "        ultra_mapping.update(mapping)\n",
    "    G=nx.union_all(list_,rename=lists_of_pathways)\n",
    "    return nx.relabel.relabel_nodes(G,ultra_mapping)\n",
    "\n",
    "\n",
    "\n",
    "def pathway_enrichment(\n",
    "    gene_list_of_interest,\n",
    "    pathways,\n",
    "    background_genes=None,\n",
    "    min_overlap=1,\n",
    "    fdr_threshold=0.01):\n",
    "    \n",
    "    # Convert to sets\n",
    "    gene_list_of_interest = set(gene_list_of_interest)\n",
    "    \n",
    "    if background_genes is None:\n",
    "        background_genes = set().union(*pathways.values()) | gene_list_of_interest\n",
    "    else:\n",
    "        background_genes = set(background_genes)\n",
    "    \n",
    "    N = len(background_genes)  # background size\n",
    "    n = len(gene_list_of_interest)  # query size\n",
    "    \n",
    "    results = []\n",
    "    for pathway_name, pathway_genes in pathways.items():\n",
    "        pathway_genes = set(pathway_genes)\n",
    "        K = len(pathway_genes)  # pathway size\n",
    "        overlapping_genes = gene_list_of_interest & pathway_genes\n",
    "        k = len(overlapping_genes)  # observed overlap\n",
    "        \n",
    "        if k < min_overlap:\n",
    "            continue\n",
    "        \n",
    "        # Hypergeometric test\n",
    "        p_value = hypergeom.sf(k - 1, N, K, n)\n",
    "        \n",
    "        # Expected overlap under null model\n",
    "        expected = (K / N) * n\n",
    "        \n",
    "        # Enrichment metrics\n",
    "        enrichment_ratio = (k / expected) if expected > 0 else np.nan\n",
    "        odds_ratio = (k / n) / (K / N) if K > 0 else np.nan\n",
    "        \n",
    "        results.append({\n",
    "            'Pathway': pathway_name,\n",
    "            'Overlap_Count': k,\n",
    "            'Pathway_Size': K,\n",
    "            'Gene_List_Size': n,\n",
    "            'Background_Size': N,\n",
    "            'P_Value': p_value,\n",
    "            'Expected_Overlap': expected,\n",
    "            'Enrichment_Ratio': enrichment_ratio,\n",
    "            'Odds_Ratio': odds_ratio,\n",
    "            '-log10(P)': -np.log10(p_value + 1e-300),  # avoid log(0)\n",
    "            'Overlapping_Genes': sorted(overlapping_genes)            \n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=[\n",
    "            'Pathway', 'Overlap_Count', 'Pathway_Size', 'Gene_List_Size', 'Background_Size',\n",
    "            'P_Value', 'Adjusted_P_Value', 'Expected_Overlap', 'Enrichment_Ratio', 'Odds_Ratio', '-log10(P)', 'Overlapping_Genes'\n",
    "        ])\n",
    "    \n",
    "    # FDR correction\n",
    "    df['Adjusted_P_Value'] = fdrcorrection(df['P_Value'])[1]\n",
    "    \n",
    "    # Filter by FDR threshold\n",
    "    df = df[df['Adjusted_P_Value'] <= fdr_threshold]\n",
    "    \n",
    "    return df.sort_values(by='Adjusted_P_Value').reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "def graph_generation(disease,background_genes,pathways,gene_in_disease,target,min_overlap,fdr_threshold):\n",
    "\n",
    "    results = pathway_enrichment(gene_in_disease,pathways,background_genes=background_genes,\n",
    "                                 min_overlap=2,fdr_threshold=0.05)\n",
    "    df=pd.DataFrame(results) \n",
    "    enriched_pathways=df['Pathway'].to_list()\n",
    "    if enriched_pathways:\n",
    "        G=make_pathway_union(enriched_pathways)\n",
    "        return G,disease,target,gene_in_disease,enriched_pathways\n",
    "    return None ,None,None,None,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "29afda7b-3513-42fb-863f-01e86ddcbb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "protienn=set(list(sys2gene.keys())+list(sys2gene.values()))\n",
    "pathways={i:v for i, v in genes_in_pathway.items() for V in v if V in protienn and i in list_of_all_graphml}\n",
    "background_genes = set(sys2gene[V] for i in genes_in_pathway.values() for V in i if V in protienn)\n",
    "\n",
    "results=[]\n",
    "processed_disease=[]\n",
    "for i,v in disease_target_aff_genes.items(): \n",
    "    for I in v.keys():\n",
    "        if i in processed_disease:\n",
    "            continue\n",
    "        processed_disease.append(i)\n",
    "        graph,disease,target,genes,enrichmed=graph_generation(disease=i,background_genes=background_genes,\n",
    "                                                    pathways=pathways,gene_in_disease=v[I],target=I,min_overlap=1,fdr_threshold=0.10)\n",
    "        \n",
    "        if graph:\n",
    "            results.append({\n",
    "                'Graph': graph,\n",
    "                'disease': disease ,   \n",
    "                'pathway':enrichmed\n",
    "            })"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4960255d-db6b-426e-aa81-30becd674b56",
   "metadata": {},
   "source": [
    "with open('/home/bionik/AI_ML/FDA_Project/Data/New/pathway_enrichment_graphs_for_graph_data.pkl', 'wb') as file:\n",
    "    pickle.dump(results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7bafb9da-9c06-4213-810e-efecc3dc4472",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_graph(G,prot_emb,sys2gene):\n",
    "    protienn=set(list(sys2gene.keys())+list(sys2gene.values()))\n",
    "    \n",
    "    for node, attr in G.nodes(data=True):\n",
    "        if not attr:\n",
    "            G.nodes[node]['label']='NA'\n",
    "        if node in protienn:\n",
    "            G.nodes[node]['label']='Protein'\n",
    "    G=nx.relabel_nodes(G,sys2gene)\n",
    "\n",
    "    for i,v,e in G.edges(data=True):\n",
    "        if not e.get('label'):\n",
    "            G[i][v]['label']='NA'\n",
    "            \n",
    "    protein_mask=[True if v.get('label')=='Protein' else False for i,v in G.nodes(data=True)]  \n",
    "    protein_mask=torch.tensor(np.array(protein_mask))\n",
    "    \n",
    "    return G,protein_mask\n",
    "    \n",
    "def get_positional_enc(G,enc_dim):\n",
    "    laplace_matrix=nx.directed_laplacian_matrix(G,walk_type='random')\n",
    "    eigvals, eigvecs = eigsh(laplace_matrix)\n",
    "    x=eigvecs[:,1:enc_dim+1]\n",
    "    id_sorted= x[:,:1].flatten().argsort()\n",
    "    return x,id_sorted\n",
    "\n",
    "def generate_graph_feature(G,Protein_embeb_dict,pos_enc_dim):\n",
    "    graph_node_features=[]\n",
    "    for i,v in G.nodes(data=True):\n",
    "        if i in Protein_embeb_dict.keys():\n",
    "            graph_node_features.append(Protein_embeb_dict[i])\n",
    "        else:\n",
    "            x=np.zeros(64,dtype=np.float32)\n",
    "            graph_node_features.append(x)\n",
    "    x,ids2=get_positional_enc(G,enc_dim=pos_enc_dim) \n",
    "    graph_node_features=np.array(graph_node_features)\n",
    "    graph_node_features=np.concat((graph_node_features,x),axis=1,dtype=np.float32)\n",
    "    return graph_node_features\n",
    "\n",
    "def generate_graph_data(G,disease_involved,pos_enc,prot_embs,edge_mapping,target,sys2gene):\n",
    "\n",
    "    # mapping: {'activates': 0, 'complexed': 1, 'inhibites': 2, 'reacts': 3, 'self': 4}\n",
    "    G,protein_mask=preprocess_graph(G,prot_emb,sys2gene)\n",
    "    x_feature=generate_graph_feature(G,prot_embs,pos_enc)\n",
    "    disease_involved_mask=np.isin(np.array(list(G.nodes())),np.array(disease_involved)).reshape(-1,1)\n",
    "    attenion_on_disease_genes=torch.tensor(np.where(disease_involved_mask,2,1))\n",
    "    y=torch.tensor(np.isin(list(G.nodes()),np.array(target)))\n",
    "    label=torch.tensor(torch.where(y,1,0),dtype=torch.long)\n",
    "    node2id={u:i for i,(u,v) in enumerate(G.nodes(data=True))}\n",
    "    G_reindexed = nx.relabel_nodes(G, node2id)\n",
    "    \n",
    "    edges=[]\n",
    "    types=[]\n",
    "    for u, v ,data in G_reindexed.edges(data=True):\n",
    "        edges.append([u,v])\n",
    "        types.append(data.get('label'))\n",
    "        \n",
    "    edge_index=np.array(edges)\n",
    "    edge_index=edge_index.T\n",
    "    edge_index=torch.tensor(edge_index) \n",
    "    types_relations=np.array(types)\n",
    "    edge_types_for_scaling=torch.tensor(np.where(types_relations=='activates',2.0,np.where(types_relations=='inhibites',0.5,1)))\n",
    "    \n",
    "    edge_index_looped, _ = add_self_loops(edge_index, num_nodes=x_feature.shape[0])\n",
    "    row, col = edge_index_looped\n",
    "    deg = degree(col, x_feature.shape[0])\n",
    "    deg_inv_sqrt = deg.pow(-0.5)\n",
    "    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "    norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "    norm=torch.tensor(norm,dtype=torch.float32)\n",
    "    remainin=x_feature.shape[0]\n",
    "    rem_for_scaling=torch.ones(remainin,dtype=torch.float32)\n",
    "    types_relations=np.concat((types_relations,np.array(['self']*remainin)),axis=0)\n",
    "    edge_types_for_scaling=torch.tensor(torch.concat((edge_types_for_scaling,rem_for_scaling),dim=-1),dtype=torch.float32)\n",
    "    types_relations=torch.tensor([edge_mapping[i] for i in types_relations],dtype=torch.long)\n",
    "\n",
    "    data=Data(x=torch.tensor(x_feature),edge_index=edge_index_looped,node_deg_norm=norm,\n",
    "             types_of_edges=types_relations,types_message_scaling=edge_types_for_scaling,\n",
    "              label=label,disease_genes=attenion_on_disease_genes,protein_mask=protein_mask)\n",
    "\n",
    "\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "37307c7a-100a-41f2-8af7-4789eee932d8",
   "metadata": {},
   "source": [
    "with open('/home/bionik/AI_ML/FDA_Project/Data/New/pathway_enrichment_graphs.pkl', 'rb') as file:\n",
    "    results = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20e4f43-15b6-4d18-a605-4eee1b9e5bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping={'activates': 0, 'complexed': 1, 'inhibites': 2, 'reacts': 3, 'self': 4}\n",
    "\n",
    "graph_list=[]\n",
    "for i in pathway_enrichment:   ## of enriched pathway data with disease, graph and pathway affectde\n",
    "    target_list=list(disease_target_aff_genes[i['disease']].keys())\n",
    "    data1=generate_graph_data(G=i['Graph'],disease_involved=disease_target_aff_genes[i['disease']][target_list[0]],\n",
    "                   pos_enc=4,prot_embs=prot_emb,edge_mapping=mapping,target=target_list,sys2gene=sys2gene)\n",
    "    data1.name=i['disease']\n",
    "    graph_list.append(data1)\n",
    "\n",
    "torch.save(graph_list,'/home/bionik/AI_ML/FDA_Project/Data/New/all_targets_graph_data_list_from_disease_target_aff_genes_enriched.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "581d4231-3380-4c61-9d8a-7e4f0f752a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_GCNConv2(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels,number_of_edges):\n",
    "        super().__init__(aggr='add')  # \"Add\" aggregation (Step 5).\n",
    "        self.bias = Parameter(torch.empty(out_channels))\n",
    "        self.transform_=torch.nn.Parameter(torch.randn(number_of_edges,in_channels,out_channels))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, edge_index,\n",
    "                edge_type_scaling, norm,edge_type):\n",
    "\n",
    "        out = self.propagate(edge_index=edge_index, x=x,\n",
    "                             norm=norm ,edge_type_scaling=edge_type_scaling,edge_type=edge_type)\n",
    "\n",
    "        return out + self.bias\n",
    "\n",
    "    def message(self, x_j, norm,edge_type_scaling, edge_type):# shape of norm is same as edge index after the self loop\n",
    "        Weights=self.transform_[edge_type]\n",
    "        transformed_x_j=torch.bmm(x_j.unsqueeze(1),Weights).squeeze(1)\n",
    "        del Weights\n",
    "        x_j=(norm.view(-1, 1) * transformed_x_j  * edge_type_scaling.view(-1,1))\n",
    "        x_j=torch.layer_norm(x_j,normalized_shape=x_j.shape)\n",
    "        return  x_j\n",
    "\n",
    "class multi_attention_transformer(torch.nn.Module):\n",
    "    def __init__(self,Feed_hidden_transfomer=64,feature_dim_input=64,heads=0,drop_out=0.2,concat=True,Type='Self'):\n",
    "        super(multi_attention_transformer,self).__init__()\n",
    "        self.heads= heads if heads else None\n",
    "        self.drop_out=drop_out\n",
    "        self.feature_dim_input=feature_dim_input\n",
    "        self.linear_T_dim_for_attention=feature_dim_input*heads if heads else feature_dim_input\n",
    "        self.concat=concat\n",
    "        self.Type=Type\n",
    "        self.Q=torch.nn.Linear(self.feature_dim_input,self.linear_T_dim_for_attention)\n",
    "        self.K=torch.nn.Linear(self.feature_dim_input,self.linear_T_dim_for_attention)\n",
    "        self.V=torch.nn.Linear(self.feature_dim_input,self.linear_T_dim_for_attention)\n",
    "        self.Linear1=torch.nn.Linear(feature_dim_input,Feed_hidden_transfomer)\n",
    "        self.Linear2=torch.nn.Linear(Feed_hidden_transfomer,feature_dim_input)\n",
    "        if self.heads:\n",
    "            self.heads=heads\n",
    "            self.head_dim=self.linear_T_dim_for_attention//self.heads\n",
    "            self.Linear_output=torch.nn.Linear(self.linear_T_dim_for_attention,self.feature_dim_input)\n",
    "        self.reset_parameter()\n",
    "\n",
    "\n",
    "    def reset_parameter(self):\n",
    "        self.Q.reset_parameters()\n",
    "        self.K.reset_parameters()\n",
    "        self.V.reset_parameters()\n",
    "        if self.heads:\n",
    "            self.Linear_output.reset_parameters()\n",
    "\n",
    "    def attention_QKV(self,Q,K,V,attention_mask):\n",
    "        feature_dim=Q.size(-1)\n",
    "        Q_K=torch.matmul(Q,K.transpose(-2,-1))/torch.tensor(feature_dim**0.5)\n",
    "        weights=F.softmax(Q_K,dim=-1)\n",
    "        weights=F.dropout(weights,p=self.drop_out)\n",
    "        output_attented=torch.matmul(weights,V)\n",
    "        return output_attented, weights\n",
    "\n",
    "    def forward(self,x1,x2,attention_mask):\n",
    "        if self.Type=='Cross':\n",
    "            Q=self.Q(x1)\n",
    "            K=self.K(x2)\n",
    "            V=self.V(x2)\n",
    "        if self.Type=='Self':\n",
    "            Q=self.Q(x1)\n",
    "            K=self.K(x1)\n",
    "            V=self.V(x1)\n",
    "\n",
    "        if self.heads:\n",
    "            self.input_size=x1.size(0)\n",
    "            #print(f\"thisis a multihead attention{Q.shape}\")\n",
    "            Q=Q.view(self.input_size,self.heads,self.head_dim).transpose(0,1)   # ## divides 1 input with 64 dim to 4 with 16 dim \n",
    "            K=K.view(self.input_size,self.heads,self.head_dim).transpose(0,1)\n",
    "            V=V.view(self.input_size,self.heads,self.head_dim).transpose(0,1)\n",
    "            '''B, N, D = x1.shape  if the batch size is more than 1\n",
    "            Q = Q.view(B, N, self.heads, self.head_dim).transpose(1, 2)  # (B, heads, N, head_dim)\n",
    "            K = K.view(B, N, self.heads, self.head_dim).transpose(1, 2)\n",
    "            V = V.view(B, N, self.heads, self.head_dim).transpose(1, 2)'''\n",
    "            out,wei=self.attention_QKV(Q,K,V,attention_mask)\n",
    "            if self.concat:\n",
    "                out=out.transpose(0,1).contiguous().view(self.input_size,self.linear_T_dim_for_attention)\n",
    "                '''out = out.transpose(1, 2).contiguous().view(B, N, self.linear_T_dim_for_attention)  if batch size is more than 1'''\n",
    "                attented_output=self.Linear_output(out)\n",
    "            else:\n",
    "                attented_output=out.mean(dim=0)\n",
    "        else:\n",
    "            attented_output, wei=self.attention_QKV(Q,K,V,attention_mask)\n",
    "\n",
    "        attented_output=F.dropout(attented_output,p=self.drop_out,training=self.training)\n",
    "        attented_output=F.layer_norm(attented_output+x1,normalized_shape=attented_output.shape)\n",
    "        Feed_forward=F.dropout(F.relu(self.Linear1(attented_output)),p=self.drop_out,training=self.training)\n",
    "        Feed_forward_out=self.Linear2(Feed_forward)\n",
    "        Feed_forward_out=F.layer_norm(attented_output+F.dropout(Feed_forward_out,p=self.drop_out,training=self.training),normalized_shape=attented_output.shape)\n",
    "        return Feed_forward_out\n",
    "\n",
    "\n",
    "\n",
    "def focal_loss(logits,labels,gamma,alpha,epsilon):\n",
    "    labels=labels.reshape(-1,1).to(dtype=torch.float32)\n",
    "    if epsilon!=0:\n",
    "        labels = labels * (1 - epsilon) + 0.5 * epsilon\n",
    "    BCE_loss=F.binary_cross_entropy_with_logits(logits,labels)\n",
    "    prob=torch.sigmoid(logits)\n",
    "    p_t=prob*labels+(1-prob)*(1-labels)\n",
    "    focal_weight=(1-p_t)**gamma\n",
    "    alpha_t=alpha*labels+(1-alpha)*(1-labels)\n",
    "    loss=alpha_t*BCE_loss* focal_weight\n",
    "    return loss.mean()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "11af3b10-d503-4eed-bf65-1880f0267fe6",
   "metadata": {},
   "source": [
    "graph_data_list_new=torch.load('/home/bionik/AI_ML/FDA_Project/Data/New/graph_data_list_from_disease_target_aff_genes_enriched.pt',weights_only=False)\n",
    "train,test,_,_=train_test_split(graph_data_list_new,[i.label for i in graph_data_list_new],train_size=.85,test_size=.15,random_state=41)\n",
    "torch.save(train,'/home/bionik/AI_ML/FDA_Project/Data/New/graph_data_list_all_targets_TRAIN_graph_data_list_from_disease_target_aff_genes_enriched.pt')\n",
    "torch.save(test,'/home/bionik/AI_ML/FDA_Project/Data/New/graph_data_list_all_targets_TEST_graph_data_list_from_disease_target_aff_genes_enriched.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfa547ca-f1b1-4f52-95d2-1e8120b486ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "diff_model_final_model(\n",
       "  (conv_disease1): Custom_GCNConv2()\n",
       "  (conv_disease2): Custom_GCNConv2()\n",
       "  (self_attention_disease): multi_attention_transformer(\n",
       "    (Q): Linear(in_features=22, out_features=44, bias=True)\n",
       "    (K): Linear(in_features=22, out_features=44, bias=True)\n",
       "    (V): Linear(in_features=22, out_features=44, bias=True)\n",
       "    (Linear1): Linear(in_features=22, out_features=32, bias=True)\n",
       "    (Linear2): Linear(in_features=32, out_features=22, bias=True)\n",
       "    (Linear_output): Linear(in_features=44, out_features=22, bias=True)\n",
       "  )\n",
       "  (cross_attention_disease): multi_attention_transformer(\n",
       "    (Q): Linear(in_features=22, out_features=44, bias=True)\n",
       "    (K): Linear(in_features=22, out_features=44, bias=True)\n",
       "    (V): Linear(in_features=22, out_features=44, bias=True)\n",
       "    (Linear1): Linear(in_features=22, out_features=32, bias=True)\n",
       "    (Linear2): Linear(in_features=32, out_features=22, bias=True)\n",
       "    (Linear_output): Linear(in_features=44, out_features=22, bias=True)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=66, out_features=32, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Dropout(p=0.35, inplace=False)\n",
       "    (3): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class diff_model_final_model(torch.nn.Module):\n",
    "    def __init__(self,input_layer,pos_enc,rels,conv_dim,output_layer,heads,feed_forward_transformer,drop_out,hidden_last_layer):\n",
    "        super(diff_model_final_model,self).__init__()\n",
    "        self.dropout=drop_out\n",
    "        self.hidden_last_layer=hidden_last_layer\n",
    "        self.input_layer=input_layer\n",
    "        self.real_input=input_layer-pos_enc+2\n",
    "        self.pos_enc=pos_enc\n",
    "        self.conv_dim=conv_dim\n",
    "        self.relations=rels\n",
    "        self.output_layer=output_layer\n",
    "        self.input_sage=int(input_layer-pos_enc)\n",
    "        self.heads=heads\n",
    "        self.feed_forward_transformer=feed_forward_transformer\n",
    "        self.input_tranformer=int(conv_dim+pos_enc+2)\n",
    "        self.conv_disease1=Custom_GCNConv2(self.real_input,self.conv_dim,self.relations)\n",
    "        self.conv_disease2=Custom_GCNConv2(self.conv_dim,self.conv_dim,self.relations)\n",
    "        self.self_attention_disease=multi_attention_transformer(self.feed_forward_transformer,self.input_tranformer,self.heads,self.dropout,True,'Self')\n",
    "        self.cross_attention_disease=multi_attention_transformer(self.feed_forward_transformer,self.input_tranformer,self.heads,self.dropout,True,'Cross')\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            Linear(self.input_tranformer*3, self.hidden_last_layer),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Dropout(self.dropout),\n",
    "            Linear(self.hidden_last_layer, self.output_layer))\n",
    "\n",
    "    def forward(self,x,edge_index,disease_genes,protein_mask,types_of_edges,norm,edge_type_scaling):\n",
    "        normal_genes=torch.where(disease_genes==2,1,torch.where(disease_genes==1,2,disease_genes))\n",
    "        normal_genes[~protein_mask]=1\n",
    "        involved_mask_disease=torch.where(disease_genes==2,1,0)\n",
    "        involved_mask_normal=torch.where(normal_genes==2,1,0)\n",
    "        disease_convolution=F.dropout(F.leaky_relu(self.conv_disease1(torch.concat([x[:,:-self.pos_enc],involved_mask_disease,involved_mask_normal],dim=1),\n",
    "                                                                      edge_index,edge_type_scaling,norm,types_of_edges)),p=self.dropout,training=self.training)\n",
    "        disease_convolution=F.dropout(F.leaky_relu(self.conv_disease2(disease_convolution,edge_index,edge_type_scaling,norm,types_of_edges)),p=self.dropout,training=self.training)\n",
    "        disease_convolution=torch.concat([involved_mask_disease,involved_mask_normal,disease_convolution,x[:,-self.pos_enc:]],dim=1)\n",
    "        disease_attended=self.self_attention_disease(disease_convolution,disease_convolution,disease_genes)\n",
    "        normal_attended=self.self_attention_disease(disease_convolution,disease_convolution,normal_genes)\n",
    "        disease_cross_attended=self.cross_attention_disease(disease_attended,normal_attended,normal_genes) + disease_convolution\n",
    "        normal_cross_attended=self.cross_attention_disease(normal_attended,disease_attended,disease_genes) + disease_convolution\n",
    "        difference=disease_cross_attended*normal_cross_attended\n",
    "        difference=torch.concat([difference,disease_cross_attended,normal_cross_attended],dim=1)\n",
    "        out=self.classifier(difference)\n",
    "        return out\n",
    "\n",
    "\n",
    "Arch_1=diff_model_final_model(input_layer=68,pos_enc=4,rels=5,output_layer=1,heads=2,\n",
    "                     feed_forward_transformer=32,drop_out=0.35,hidden_last_layer=32,conv_dim=16)\n",
    "Arch_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27f5bb0e-7082-4bb0-b0ba-3621124bec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,loader,device,gamma,alpha,Focal_loss,max_norm,epsilon):\n",
    "    loss_per_epoch=0\n",
    "    model.train()\n",
    "    for data in tqdm.tqdm(loader):\n",
    "        if sum(data.label)==0:   ## some graphs dont have targets\n",
    "            continue\n",
    "        optimizer.zero_grad()\n",
    "        with autocast(device_type='cuda',dtype=torch.float16):\n",
    "          data=data.to(device)\n",
    "          model_out=model(data.x,data.edge_index,data.disease_genes,data.protein_mask,data.types_of_edges,data.node_deg_norm,data.types_message_scaling)\n",
    "          if Focal_loss:\n",
    "              loss=focal_loss(model_out[data.protein_mask],data.label[data.protein_mask],gamma,alpha,epsilon)\n",
    "          else:\n",
    "              pos_weight=sum(data.label[data.protein_mask]==0)//sum(data.label[data.protein_mask]==1)\n",
    "              loss=F.binary_cross_entropy_with_logits(model_out[data.protein_mask],data.label[data.protein_mask].reshape(-1,1).to(dtype=torch.float32),\n",
    "                                                      pos_weight=torch.tensor(pos_weight))\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        loss_per_epoch+=loss.item()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        del data,model_out\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return loss_per_epoch/len(loader)\n",
    "\n",
    "def test_model(model,loader,device,gamma,alpha,Focal_loss,threshold,epsilon):\n",
    "    model.eval()\n",
    "    loss_per_epoch=0\n",
    "    predictions_all=[]\n",
    "    labels_all=[]\n",
    "    probability_all=[]\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm.tqdm(loader):\n",
    "            if sum(data.label)==0:   ## some graphs dont have targets\n",
    "                continue\n",
    "            with autocast(device_type='cuda',dtype=torch.float16):\n",
    "              data=data.to(device)\n",
    "              model_out=model(data.x,data.edge_index,data.disease_genes,data.protein_mask,data.types_of_edges,data.node_deg_norm,data.types_message_scaling)\n",
    "              if Focal_loss:\n",
    "                  loss=focal_loss(model_out[data.protein_mask],data.label[data.protein_mask],gamma,alpha,epsilon)\n",
    "              else:\n",
    "                  pos_weight=sum(data.label[data.protein_mask]==0)//sum(data.label[data.protein_mask]==1)\n",
    "                  loss=F.binary_cross_entropy_with_logits(model_out[data.protein_mask],data.label[data.protein_mask].reshape(-1,1).to(dtype=torch.float32),\n",
    "                                                        pos_weight=torch.tensor(pos_weight))\n",
    "            loss_per_epoch+=loss.item()\n",
    "            probability=F.sigmoid(model_out)\n",
    "            predictions=(probability>threshold).long()\n",
    "            probability_all.extend(probability[data.protein_mask].cpu())\n",
    "            predictions_all.extend(predictions[data.protein_mask].cpu())\n",
    "            labels_all.extend(data.label[data.protein_mask].cpu())\n",
    "            del data,model_out\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    return loss_per_epoch/len(loader),np.array(labels_all),np.array(probability_all),np.array(predictions_all)\n",
    "\n",
    "\n",
    "def inference_model(model,data,device,top_k,node_2_id,threshold):\n",
    "    model.eval()   # node_2_id={0:\"A\",1:\"B\",2:\"C\"}\n",
    "    with torch.no_grad():\n",
    "        data=data.to(device)\n",
    "        model_output=model(data.x,data.edge_index,data.types_of_edges,data.disease_genes)\n",
    "        probabilities=F.sigmoid(model_output)\n",
    "        predicted_class=(probabilities>threshold).long()\n",
    "        sorted_prob=torch.argsort(probabilities,dim=0,descending=True)\n",
    "        top_k_targets=[node_2_id[int(i)] for i in ind]\n",
    "    return probabilities,top_k_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c0efa4-7e90-4b74-83c5-fc3281d679dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [00:39<00:00,  3.31it/s]\n",
      "100%|██████████| 94/94 [00:27<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with : 1\n",
      "Done with adding scalers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_53582/379340046.py:101: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  writer.add_pr_curve(f'TopK_{fold}/Precision_recall_curve',np.array(label),np.array(prob).flatten(),global_step=i)\n",
      "/tmp/ipykernel_53582/3421821254.py:57: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  Q_K=torch.matmul(Q,K.transpose(-2,-1))/torch.tensor(feature_dim**0.5)\n",
      "/tmp/ipykernel_53582/3421821254.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Q_K=torch.matmul(Q,K.transpose(-2,-1))/torch.tensor(feature_dim**0.5)\n",
      "/home/bionik/miniconda3/envs/datsci/lib/python3.12/site-packages/torch/jit/_trace.py:1307: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
      "Tensor-likes are not close!\n",
      "\n",
      "Mismatched elements: 1404 / 1410 (99.6%)\n",
      "Greatest absolute difference: 5.231273651123047 at index (9, 0) (up to 1e-05 allowed)\n",
      "Greatest relative difference: 1.0816517319894063 at index (9, 0) (up to 1e-05 allowed)\n",
      "  _check_trace(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 23/130 [00:06<00:31,  3.43it/s]"
     ]
    }
   ],
   "source": [
    "train_data_list=torch.load('/home/bionik/AI_ML/FDA_Project/Data/New/graph_data_list_all_targets_TRAIN_graph_data_list_from_disease_target_aff_genes_enriched.pt',weights_only=False)\n",
    "train_data_list=train_data_list[:2]\n",
    "epsilon=0.1\n",
    "threshold=0.1\n",
    "num_folds = 2\n",
    "gamma=2.0\n",
    "alpha=0.975\n",
    "Focal_loss=True\n",
    "rels=5\n",
    "epochs=40\n",
    "max_norm=3.0\n",
    "weight_decay=0.000075\n",
    "drop_out=0.20\n",
    "lr=0.0001\n",
    "input_layer=68\n",
    "conv_dim=26\n",
    "output_layer=1\n",
    "pos_enc=4\n",
    "heads=1\n",
    "feed_forward_transformer=32\n",
    "hidden_last_layer=16\n",
    "\n",
    "writer = SummaryWriter(log_dir=f'/home/bionik/TensorBoard/New_batching/')\n",
    "writer.add_text('Parameters',str(f\" Gamma: {gamma} \\nAlpha: {alpha} \\nLearning_rate: {lr} \\nWeight_decay: {weight_decay} \\nMax_norm: {max_norm} \\n\\\n",
    "Dropout: {drop_out} \\nEpochs: {epochs} \\nFocal_loss: {Focal_loss} \\nthreshold: {threshold} \\ninput_dim: {input_layer} \\n\\\n",
    "output_layer: {output_layer} \\nheads: {heads} \\nfeed_forward_transformer: {feed_forward_transformer} \\n\\\n",
    "hidden_last_layer: {hidden_last_layer} \\nepsilon: {epsilon} \\nrelation: {rels} \\npos_enc: {pos_enc}\"))\n",
    "writer.flush()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "for fold, (train_idx, validate_idx) in enumerate(kf.split(X=train_data_list)):\n",
    "    train_graphs = [train_data_list[i] for i in train_idx]\n",
    "    val_graphs = [train_data_list[i] for i in validate_idx]\n",
    "    super_graph_train = Batch.from_data_list(train_graphs)\n",
    "    sampler_train=ImbalancedSampler(super_graph_train.label,input_nodes=super_graph_train.protein_mask.int().nonzero().flatten())\n",
    "    train_loader = NeighborLoader(\n",
    "        super_graph_train,\n",
    "        input_nodes=super_graph_train.protein_mask.int().nonzero().flatten(),\n",
    "        num_neighbors=[-1,-1,-1],  # Samples up to 50 neighbors for 1 hop\n",
    "        batch_size=32,       # Number of seed nodes per mini-batch\n",
    "        sampler=sampler_train,\n",
    "        replace=False)\n",
    "\n",
    "    super_graph_val = Batch.from_data_list(val_graphs)\n",
    "    sampler_test=ImbalancedSampler(super_graph_val.label,input_nodes=super_graph_val.protein_mask.int().nonzero().flatten())\n",
    "    val_loader = NeighborLoader(\n",
    "        super_graph_val,\n",
    "        input_nodes=super_graph_val.protein_mask.int().nonzero().flatten(),\n",
    "        num_neighbors=[-1,-1,-1],  # Samples up to 50 neighbors for 1 hop\n",
    "        batch_size=32,       # Number of seed nodes per mini-batch\n",
    "        sampler=sampler_test,\n",
    "        replace=False)\n",
    "    \n",
    "    \n",
    "    scaler=GradScaler()\n",
    "    model=diff_model_final_model(input_layer=input_layer,pos_enc=pos_enc,rels=rels,conv_dim=conv_dim,output_layer=output_layer,heads=heads,\n",
    "                     feed_forward_transformer=feed_forward_transformer,drop_out=drop_out,\n",
    "                     hidden_last_layer=hidden_last_layer)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    model.to(device)\n",
    "\n",
    "\n",
    "    for i in range(1,epochs+1):\n",
    "        print('Doing:',i)\n",
    "        data_=train_loader([0]).to(device)\n",
    "        train_loss=train_model(model,train_loader,device,gamma,alpha,Focal_loss,max_norm,epsilon)\n",
    "        test_loss,label,prob,pred=test_model(model,val_loader,device,gamma,alpha,Focal_loss,threshold,epsilon)\n",
    "        print('Done with :',i)\n",
    "        f1_=f1_score(label,pred,zero_division=0)\n",
    "        f2_=fbeta_score(label,pred,beta=2,average='binary')\n",
    "        recall=recall_score(label,pred,zero_division=0)\n",
    "        accuracy=accuracy_score(label,pred)\n",
    "        precision=precision_score(label,pred,zero_division=0)\n",
    "        roc=roc_auc_score(label,prob)\n",
    "        avr_pr=average_precision_score(label,prob)\n",
    "        \n",
    "\n",
    "        writer.add_pr_curve(f'fold_{fold}/Precision_recall_curve',np.array(label),np.array(prob).flatten(),global_step=i)\n",
    "        writer.add_scalar(f'fold_{fold}/Avg_Precision_recall', avr_pr, global_step=i)\n",
    "        writer.add_scalar(f'fold_{fold}/Recall', recall, global_step=i)\n",
    "        writer.add_scalar(f'fold_{fold}/ROC', roc, global_step=i)\n",
    "        writer.add_scalar(f'fold_{fold}/Accuracy', accuracy, global_step=i)\n",
    "        writer.add_scalar(f'fold_{fold}/Precision', precision, global_step=i)\n",
    "        writer.add_scalar(f'fold_{fold}/F1 Score', f1_, global_step=i)\n",
    "        writer.add_scalar(f'fold_{fold}/F2 Score', f2_, global_step=i)\n",
    "        writer.add_scalars(f'fold_{fold}/Loss',{'train':train_loss,'test':test_loss},global_step=i)\n",
    "        print('Done with adding scalers')\n",
    "        \n",
    "        prob,idx=torch.topk(torch.tensor(prob).flatten(),k=20)\n",
    "        pred=pred[idx]\n",
    "        label=label[idx]\n",
    "        f1_=f1_score(label,pred,zero_division=0)\n",
    "        f2_=fbeta_score(label,pred,beta=2,average='binary')\n",
    "        recall=recall_score(label,pred,zero_division=0)\n",
    "        accuracy=accuracy_score(label,pred)\n",
    "        precision=precision_score(label,pred,zero_division=0)\n",
    "        roc=roc_auc_score(label,prob)\n",
    "        avr_pr=average_precision_score(label,prob)\n",
    "        writer.add_pr_curve(f'TopK_{fold}/Precision_recall_curve',np.array(label),np.array(prob).flatten(),global_step=i)\n",
    "        writer.add_scalar(f'TopK_{fold}/Avg_Precision_recall', avr_pr, global_step=i)\n",
    "        writer.add_scalar(f'TopK_{fold}/Recall', recall, global_step=i)\n",
    "        writer.add_scalar(f'TopK_{fold}/ROC', roc, global_step=i)\n",
    "        writer.add_scalar(f'TopK_{fold}/Accuracy', accuracy, global_step=i)\n",
    "        writer.add_scalar(f'TopK_{fold}/Precision', precision, global_step=i)\n",
    "        writer.add_scalar(f'TopK_{fold}/F1 Score', f1_, global_step=i)\n",
    "        writer.add_scalar(f'TopK_{fold}/F2 Score', f2_, global_step=i)\n",
    "        del label,prob,pred, train_loss,test_loss\n",
    "\n",
    "\n",
    "        for name,para in model.named_parameters():\n",
    "            writer.add_histogram(f\"fold_{fold}/Param_{name}_weights\",para,i)\n",
    "\n",
    "        if fold==0 and i==1:\n",
    "            writer.add_text('Model Architecture', str(model))\n",
    "            writer.add_graph(model,(data_.x,data_.edge_index,data_.disease_genes,data_.protein_mask,\n",
    "                                   data_.types_of_edges,data_.node_deg_norm,data_.types_message_scaling))\n",
    "            del data_\n",
    "        writer.flush()\n",
    "\n",
    "\n",
    "writer.flush()\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9212c62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data_list=torch.load('/home/bionik/AI_ML/FDA_Project/Data/New/graph_data_list_all_targets_TRAIN_graph_data_list_from_disease_target_aff_genes_enriched.pt',weights_only=False)\n",
    "epsilon=0.1\n",
    "threshold=0.1\n",
    "num_folds = 2\n",
    "gamma=2.0\n",
    "alpha=0.975\n",
    "Focal_loss=True\n",
    "rels=5\n",
    "epochs=40\n",
    "max_norm=3.0\n",
    "weight_decay=0.000075\n",
    "drop_out=0.20\n",
    "lr=0.0001\n",
    "input_layer=68\n",
    "conv_dim=26\n",
    "output_layer=1\n",
    "pos_enc=4\n",
    "heads=1\n",
    "feed_forward_transformer=32\n",
    "hidden_last_layer=16\n",
    "\n",
    "writer = SummaryWriter(log_dir=f'/home/bionik/TensorBoard/New_batching/')\n",
    "writer.add_text('Parameters',str(f\" Gamma: {gamma} \\nAlpha: {alpha} \\nLearning_rate: {lr} \\nWeight_decay: {weight_decay} \\nMax_norm: {max_norm} \\n\\\n",
    "Dropout: {drop_out} \\nEpochs: {epochs} \\nFocal_loss: {Focal_loss} \\nthreshold: {threshold} \\ninput_dim: {input_layer} \\n\\\n",
    "output_layer: {output_layer} \\nheads: {heads} \\nfeed_forward_transformer: {feed_forward_transformer} \\n\\\n",
    "hidden_last_layer: {hidden_last_layer} \\nepsilon: {epsilon} \\nrelation: {rels} \\npos_enc: {pos_enc}\"))\n",
    "writer.flush()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "for fold, (train_idx, validate_idx) in enumerate(kf.split(X=train_data_list)):\n",
    "    train_graphs = [train_data_list[i] for i in train_idx]\n",
    "    val_graphs = [train_data_list[i] for i in validate_idx]\n",
    "    train_loader = torch_geometric.loader.DataLoader(train_graphs, batch_size=1, shuffle=True)\n",
    "    val_loader = torch_geometric.loader.DataLoader(val_graphs, batch_size=1, shuffle=False)\n",
    "    \n",
    "    \n",
    "    scaler=GradScaler()\n",
    "    model=diff_model_final_model(input_layer=input_layer,pos_enc=pos_enc,rels=rels,conv_dim=conv_dim,output_layer=output_layer,heads=heads,\n",
    "                     feed_forward_transformer=feed_forward_transformer,drop_out=drop_out,\n",
    "                     hidden_last_layer=hidden_last_layer)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    model.to(device)\n",
    "\n",
    "\n",
    "    for i in range(1,epochs+1):\n",
    "        print('Doing:',i)\n",
    "        train_loss=train_model(model,train_loader,device,gamma,alpha,Focal_loss,max_norm,epsilon)\n",
    "        test_loss,label,prob,pred=test_model(model,val_loader,device,gamma,alpha,Focal_loss,threshold,epsilon)\n",
    "        print('Done with :',i)\n",
    "        f1_=f1_score(label,pred,zero_division=0)\n",
    "        f2_=fbeta_score(label,pred,beta=2,average='binary')\n",
    "        recall=recall_score(label,pred,zero_division=0)\n",
    "        accuracy=accuracy_score(label,pred)\n",
    "        precision=precision_score(label,pred,zero_division=0)\n",
    "        roc=roc_auc_score(label,prob)\n",
    "        avr_pr=average_precision_score(label,prob)\n",
    "        \n",
    "\n",
    "        writer.add_pr_curve(f'fold_{fold}/Precision_recall_curve',np.array(label),np.array(prob).flatten(),global_step=i)\n",
    "        writer.add_scalar(f'fold_{fold}/Avg_Precision_recall', avr_pr, global_step=i)\n",
    "        writer.add_scalar(f'fold_{fold}/Recall', recall, global_step=i)\n",
    "        writer.add_scalar(f'fold_{fold}/ROC', roc, global_step=i)\n",
    "        writer.add_scalar(f'fold_{fold}/Accuracy', accuracy, global_step=i)\n",
    "        writer.add_scalar(f'fold_{fold}/Precision', precision, global_step=i)\n",
    "        writer.add_scalar(f'fold_{fold}/F1 Score', f1_, global_step=i)\n",
    "        writer.add_scalar(f'fold_{fold}/F2 Score', f2_, global_step=i)\n",
    "        writer.add_scalars(f'fold_{fold}/Loss',{'train':train_loss,'test':test_loss},global_step=i)\n",
    "        print('Done with adding scalers')\n",
    "        \n",
    "        prob,idx=torch.topk(torch.tensor(prob).flatten(),k=20)\n",
    "        pred=pred[idx]\n",
    "        label=label[idx]\n",
    "        f1_=f1_score(label,pred,zero_division=0)\n",
    "        f2_=fbeta_score(label,pred,beta=2,average='binary')\n",
    "        recall=recall_score(label,pred,zero_division=0)\n",
    "        accuracy=accuracy_score(label,pred)\n",
    "        precision=precision_score(label,pred,zero_division=0)\n",
    "        roc=roc_auc_score(label,prob)\n",
    "        avr_pr=average_precision_score(label,prob)\n",
    "        writer.add_pr_curve(f'TopK_{fold}/Precision_recall_curve',np.array(label),np.array(prob).flatten(),global_step=i)\n",
    "        writer.add_scalar(f'TopK_{fold}/Avg_Precision_recall', avr_pr, global_step=i)\n",
    "        writer.add_scalar(f'TopK_{fold}/Recall', recall, global_step=i)\n",
    "        writer.add_scalar(f'TopK_{fold}/ROC', roc, global_step=i)\n",
    "        writer.add_scalar(f'TopK_{fold}/Accuracy', accuracy, global_step=i)\n",
    "        writer.add_scalar(f'TopK_{fold}/Precision', precision, global_step=i)\n",
    "        writer.add_scalar(f'TopK_{fold}/F1 Score', f1_, global_step=i)\n",
    "        writer.add_scalar(f'TopK_{fold}/F2 Score', f2_, global_step=i)\n",
    "        del label,prob,pred, train_loss,test_loss\n",
    "        \n",
    "\n",
    "        for name,para in model.named_parameters():\n",
    "            writer.add_histogram(f\"fold_{fold}/Param_{name}_weights\",para,i)\n",
    "\n",
    "        if fold==0 and i==1:\n",
    "            writer.add_text('Model Architecture', str(model))\n",
    "            writer.add_graph(model,(data_.x,data_.edge_index,data_.disease_genes,data_.protein_mask,\n",
    "                                   data_.types_of_edges,data_.node_deg_norm,data_.types_message_scaling))\n",
    "            del data_\n",
    "        writer.flush()\n",
    "\n",
    "\n",
    "writer.flush()\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9880a71-a37c-4899-8c5d-e7d2735cb086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "772539ca-8092-4982-a397-11b80df00339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-4ce287f673a0a250\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-4ce287f673a0a250\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /home/bionik/TensorBoard/New_batching/"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f64f6653-beb7-4b02-8156-fcf9103147c2",
   "metadata": {},
   "source": [
    "Find optimal threshold for the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49ba2ba-860b-42ff-94d5-c4a3b92dda9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label=[]\n",
    "prob=[]\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        data=data.to(device)\n",
    "        model_out=model(data.x,data.edge_index,data.disease_genes,data.protein_mask,data.types_of_edges,data.node_deg_norm,data.types_message_scaling)\n",
    "        probab=F.sigmoid(model_out)\n",
    "        labels=data.label[data.protein_mask].reshape(-1,1)\n",
    "        label.extend(labels.cpu().flatten())\n",
    "        prob.extend(probab.cpu().flatten())\n",
    "\n",
    "thresholds = arange(0, 0.3, 0.05)\n",
    "f1_scores=[]\n",
    "for t in thresholds:\n",
    "    prediction=(prob>t).int()\n",
    "    f1_=f1_score(label,prediction)\n",
    "    f1_scores.append(f1_)\n",
    "\n",
    "best_threshold=thresholds[argmax(scores)]\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(label, prob)\n",
    "fscore = (2 * precision * recall) / (precision + recall)\n",
    "best_threshold=thresholds[argmax(fscore)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34358fc",
   "metadata": {},
   "source": [
    "AFTER ADDING SUBGRAPH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b035e30d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a81529f-becc-4c8b-96bd-45df1506aa75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/home/bionik/AI_ML/FDA_Project/Data/New/subgraph_all_pathway_enrichment_graphs.pkl', 'rb') as file:\n",
    "    pathway_enrichment = pickle.load(file)\n",
    "    \n",
    "with open('/home/bionik/AI_ML/FDA_Project/Data/New/Protein_embeddings.pkl', 'rb') as file:\n",
    "    prot_emb = pickle.load(file)\n",
    "    \n",
    "with open('/home/bionik/AI_ML/FDA_Project/Data/New/pathways_for_whole_G.pkl', 'rb') as file:\n",
    "    gra = pickle.load(file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcf0ecd-ccb1-45da-9245-caab07a9a9dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "Whole_G=make_pathway_union(gra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a94573-a463-409d-9d73-9c08f0c6cfa9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Whole_G=nx.subgraph(Whole_G,nbunch=list(nx.weakly_connected_components(Whole_G))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ed0b50-bcf6-4ed0-878a-948c1ecc33ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nx.write_graphml(Whole_G,'/home/bionik/AI_ML/FDA_Project/Data/Graphs/Whole.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0959e472-66ce-4607-946f-6a2df734fab0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Whole_G=nx.read_graphml('/home/bionik/AI_ML/FDA_Project/Data/Graphs/Whole.graphml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32258f98-a0e6-4702-a148-6c4da85a27e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "edge_mapping={'activates': 0, 'complexed': 1, 'inhibites': 2, 'reacts': 3, 'self': 4}\n",
    "protienn=set(prot_emb.keys())\n",
    "\n",
    "Whole_G,protein_mask,id_2_node=preprocess_graph(Whole_G,protienn)\n",
    "x_feature=generate_graph_feature(Whole_G,prot_emb,4)\n",
    "nodes=list(Whole_G.nodes())\n",
    "node2id={i:v for v,i in id_2_node.items()}\n",
    "G_reindexed = nx.relabel_nodes(Whole_G, node2id)\n",
    "edges=[]\n",
    "types=[]\n",
    "for u, v ,data in G_reindexed.edges(data=True):\n",
    "    edges.append([u,v])\n",
    "    types.append(data.get('label'))\n",
    "    \n",
    "edge_index=np.array(edges)\n",
    "edge_index=edge_index.T\n",
    "edge_index=torch.tensor(edge_index) \n",
    "types_relations=np.array(types)\n",
    "edge_types_for_scaling=torch.tensor(np.where(types_relations=='activates',2.0,np.where(types_relations=='inhibites',0.5,1)))\n",
    "\n",
    "edge_index_looped, _ = add_self_loops(edge_index, num_nodes=G_reindexed.number_of_nodes())\n",
    "row, col = edge_index_looped\n",
    "deg = degree(col, x_feature.shape[0])\n",
    "deg_inv_sqrt = deg.pow(-0.5)\n",
    "deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "norm=torch.tensor(norm,dtype=torch.float32)\n",
    "remainin=G_reindexed.number_of_nodes()\n",
    "rem_for_scaling=torch.ones(remainin,dtype=torch.float32)\n",
    "types_relations=np.concat((types_relations,np.array(['self']*remainin)),axis=0)\n",
    "edge_types_for_scaling=torch.tensor(torch.concat((edge_types_for_scaling,rem_for_scaling),dim=-1),dtype=torch.float32)\n",
    "types_relations=torch.tensor([edge_mapping[i] for i in types_relations],dtype=torch.long)\n",
    "\n",
    "print('done')\n",
    "graph_list=[]\n",
    "for i in pathway_enrichment:\n",
    "    disease_involved_mask=np.isin(np.array(nodes),np.array(list(i['Genes']))).reshape(-1,1)\n",
    "    attenion_on_disease_genes=torch.tensor(np.where(disease_involved_mask,2,1))\n",
    "    y=torch.tensor(np.isin(np.array(nodes),np.array(list(i['Targets']))))\n",
    "    label=torch.tensor(torch.where(y,1,0),dtype=torch.long)\n",
    "    data=Data(label=label,disease_genes=attenion_on_disease_genes,name=i['name'])\n",
    "    graph_list.append(data)\n",
    "\n",
    "graph_data=Data(x=torch.tensor(x_feature),edge_index_looped=edge_index_looped,edge_index=edge_index,protein_mask=protein_mask,\n",
    "                node_deg_norm=norm,types_of_edges=types_relations,types_message_scaling=edge_types_for_scaling,node_mapping=node2id)\n",
    "torch.save(graph_data,'/home/bionik/AI_ML/FDA_Project/Data/New/feature_edgeindex.pt')\n",
    "\n",
    "total_graph=[]\n",
    "for i in graph_list:\n",
    "    if i.label.sum()>=1:\n",
    "        total_graph.append(i)\n",
    "    \n",
    "torch.save(total_graph,'/home/bionik/AI_ML/FDA_Project/Data/New/target_genes_mask.pt') ## has disease which dont have targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836d4b87-fe0c-4046-a0f6-a99b48eef14a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "graph_list=torch.load('/home/bionik/AI_ML/FDA_Project/Data/New/target_genes_mask.pt',weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d643d603-2b1e-45e2-b15a-81d95dd8d976",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "lables=[i.label.sum().item() for i in total_graph] # or graph_list\n",
    "test_tagerts=round(sum(lables)*0.15)\n",
    "test_graphs=round(len(total_graph)*0.15)\n",
    "random.shuffle(graph_list)\n",
    "test_data=random.choices(graph_list,k=test_graphs)\n",
    "lables=[i.label.sum().item() for i in test_data]\n",
    "print(sum(lables),test_tagerts)\n",
    "test_name=set((i.name for i in test_data))\n",
    "train_data=[i for i in total_graph if i.name not in test_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8bbcba-766e-4548-ab7b-6794fac3c16d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.save(train_data,'/home/bionik/AI_ML/FDA_Project/Data/New/Train_all_targets_graph_data_list_from_disease_target_aff_genes_enriched.pt') \n",
    "torch.save(test_data,'/home/bionik/AI_ML/FDA_Project/Data/New/Test_all_targets_graph_data_list_from_disease_target_aff_genes_enriched.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5b9fd3-74d0-4adc-ba42-428e1d1667c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_data=torch.load('/home/bionik/AI_ML/FDA_Project/Data/New/Train_all_targets_graph_data_list_from_disease_target_aff_genes_enriched.pt',weights_only=False)\n",
    "graph_data=torch.load('/home/bionik/AI_ML/FDA_Project/Data/New/feature_edgeindex.pt',weights_only=False)\n",
    "G=nx.read_graphml('/home/bionik/AI_ML/FDA_Project/Data/Graphs/Whole.graphml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5b92f2-07bb-4d43-84d3-67767644b046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random  ## do same for test graph data\n",
    "tar=0\n",
    "max=0\n",
    "from torch_geometric.data import Data\n",
    "dataaaa=[]\n",
    "mapping_node={v:i for i,v in graph_data.node_mapping.items()}\n",
    "\n",
    "di=set()\n",
    "for I in whole_data:\n",
    "    count=0\n",
    "    for Y in I.label.nonzero().flatten():\n",
    "        subset,edge_index,map,edge_mask=torch_geometric.utils.k_hop_subgraph(Y.item(),12,graph_data.edge_index_looped,relabel_nodes=True)\n",
    "        if subset.shape[0]>18000:\n",
    "            subset,edge_index,map,edge_mask=torch_geometric.utils.k_hop_subgraph(Y.item(),10,graph_data.edge_index_looped,relabel_nodes=True)\n",
    "        if subset.shape[0]>18000:\n",
    "            subset,edge_index,map,edge_mask=torch_geometric.utils.k_hop_subgraph(Y.item(),9,graph_data.edge_index_looped,relabel_nodes=True)\n",
    "        if subset.shape[0]>18000:\n",
    "            subset,edge_index,map,edge_mask=torch_geometric.utils.k_hop_subgraph(Y.item(),8,graph_data.edge_index_looped,relabel_nodes=True)\n",
    "        if edge_index.shape[1]>5000 and subset.shape[0]<18000 and I.label[subset].sum()>=2 and subset.shape[0]>7500:\n",
    "            count+=I.label[subset].sum()//3.5\n",
    "            tar+=1\n",
    "            max+=1\n",
    "            Found=True\n",
    "            di.add(I.name)\n",
    "            print(I.name,subset.shape,mapping_node[Y.item()])\n",
    "            dataaaa.append(Data(edge_mask=edge_mask,edge_index=edge_index,subset=subset,name=I.name,label=I.label[subset],disease_genes=I.disease_genes[subset]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042e82e3-2024-419c-aa5e-ca26b7ef138c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.save(dataaaa,'/home/bionik/AI_ML/FDA_Project/Data/New/Subgraph_Train_all_targets_graph_data_list_from_disease_target_aff_genes_enriched.pt') \n",
    "# same code test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0b1689-6da4-439b-b9d0-6e4c34498a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class diff_model_final_model(torch.nn.Module):\n",
    "    def __init__(self,input_layer,pos_enc,rels,conv_dim,output_layer,heads,feed_forward_transformer,drop_out,hidden_last_layer):\n",
    "        super(diff_model_final_model,self).__init__()\n",
    "        self.dropout=drop_out\n",
    "        self.hidden_last_layer=hidden_last_layer\n",
    "        self.input_layer=input_layer\n",
    "        self.real_input=input_layer-pos_enc+2\n",
    "        self.pos_enc=pos_enc\n",
    "        self.conv_dim=conv_dim\n",
    "        self.relations=rels\n",
    "        self.output_layer=output_layer\n",
    "        self.input_sage=int(input_layer-pos_enc)\n",
    "        self.heads=heads\n",
    "        self.feed_forward_transformer=feed_forward_transformer\n",
    "        self.input_tranformer=int(conv_dim+pos_enc+2)\n",
    "        self.conv_disease1=torch_geometric.nn.RGCNConv(self.real_input,self.conv_dim,self.relations)\n",
    "        self.conv_disease2=torch_geometric.nn.RGCNConv(self.conv_dim,self.conv_dim,self.relations)\n",
    "        self.self_attention_disease=multi_attention_transformer(self.feed_forward_transformer,self.input_tranformer,self.heads,self.dropout,True,'Self')\n",
    "        self.cross_attention_disease=multi_attention_transformer(self.feed_forward_transformer,self.input_tranformer,self.heads,self.dropout,True,'Cross')\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            Linear(self.input_tranformer*3, self.hidden_last_layer),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Dropout(self.dropout),\n",
    "            Linear(self.hidden_last_layer, self.output_layer))\n",
    "\n",
    "    def forward(self,x,edge_index,disease_genes,protein_mask,types_of_edges,norm,edge_type_scaling,batch):\n",
    "        normal_genes=torch.where(disease_genes==2,1,torch.where(disease_genes==1,2,disease_genes))\n",
    "        normal_genes[~protein_mask]=1\n",
    "        involved_mask_disease=torch.where(disease_genes==2,1,0)\n",
    "        involved_mask_normal=torch.where(normal_genes==2,1,0)\n",
    "        disease_convolution=F.dropout(F.leaky_relu(self.conv_disease1(torch.concat([x[:,:-self.pos_enc],involved_mask_disease,involved_mask_normal],dim=1),\n",
    "                                                                      edge_index,types_of_edges)),p=self.dropout,training=self.training)\n",
    "        normal_convolution=F.dropout(F.leaky_relu(self.conv_disease1(torch.concat([x[:,:-self.pos_enc],involved_mask_normal,involved_mask_disease],dim=1),\n",
    "                                                                      edge_index,types_of_edges)),p=self.dropout,training=self.training)\n",
    "        disease_convolution=F.dropout(F.leaky_relu(self.conv_disease2(disease_convolution,edge_index,types_of_edges)),p=self.dropout,training=self.training)\n",
    "        normal_convolution=F.dropout(F.leaky_relu(self.conv_disease2(normal_convolution,edge_index,types_of_edges)),p=self.dropout,training=self.training)\n",
    "        disease_convolution=torch.concat([involved_mask_disease,involved_mask_normal,disease_convolution,x[:,-self.pos_enc:]],dim=1)\n",
    "        normal_convolution=torch.concat([involved_mask_normal,involved_mask_disease,normal_convolution,x[:,-self.pos_enc:]],dim=1)\n",
    "        disease_attended=self.self_attention_disease(disease_convolution,disease_convolution,disease_genes)\n",
    "        normal_attended=self.self_attention_disease(normal_convolution,normal_convolution,normal_genes)\n",
    "        disease_cross_attended=self.cross_attention_disease(disease_attended,normal_attended,normal_genes)\n",
    "        normal_cross_attended=self.cross_attention_disease(normal_attended,disease_attended,disease_genes)\n",
    "        disease_global_mean=torch_geometric.nn.global_mean_pool(disease_cross_attended, batch)\n",
    "        normal_global_mean=torch_geometric.nn.global_mean_pool(normal_cross_attended, batch)\n",
    "        normal_cross_attended=normal_cross_attended+normal_global_mean\n",
    "        disease_cross_attended=disease_cross_attended+disease_global_mean\n",
    "        difference=F.layer_norm(disease_cross_attended*normal_cross_attended,normalized_shape=normal_cross_attended.shape)\n",
    "        difference=torch.concat([disease_attended,normal_attended,difference],dim=1)\n",
    "        out=self.classifier(difference)\n",
    "        return out\n",
    "\n",
    "\n",
    "Arch_1=diff_model_final_model(input_layer=68,pos_enc=4,rels=5,output_layer=1,heads=2,\n",
    "                     feed_forward_transformer=32,drop_out=0.35,hidden_last_layer=32,conv_dim=16)\n",
    "Arch_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4be8f5e-5dff-44f8-9974-3df24a0a4a22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data_list=torch.load('/home/bionik/AI_ML/FDA_Project/Data/New/Subgraph_Train_all_targets_graph_data_list_from_disease_target_aff_genes_enriched.pt',weights_only=False)\n",
    "graph_data=torch.load('/home/bionik/AI_ML/FDA_Project/Data/New/feature_edgeindex.pt',weights_only=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5ad6dd-79da-45ea-8a27-cea189986dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "diseases=list(set([i.name for i in train_data_list]))\n",
    "val_disease=list(set(random.choices(diseases,k=round(len(diseases)*0.20))))\n",
    "train_disease=list(set(diseases).difference(list(val_disease)))\n",
    "train_data=[i for i in train_data_list if i.name in train_disease]\n",
    "val_data=[i for i in train_data_list if i.name in val_disease]\n",
    "total_target_all=sum([i.label.nonzero().shape[0] for i in train_data_list])\n",
    "total_target_train=sum([i.label.nonzero().shape[0] for i in train_data])\n",
    "total_target_val=sum([i.label.nonzero().shape[0] for i in val_data])\n",
    "print(len(train_data)/len(train_data_list),total_target_train/total_target_all)\n",
    "print(len(val_data)/len(train_data_list),total_target_val/total_target_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eeb6f8-339b-4c2e-b156-9c1ecc83c478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data of subgraph whole graph data is stored in graph and data and specific subset is accessed by train_data subset\n",
    "def train_list(model,graph_data,train_dataaaa,device,gamma,alpha,Focal_loss,max_norm,epsilon,L1_Reg):\n",
    "    diseases=list(set([i.name for i in train_dataaaa]))\n",
    "    model.train()\n",
    "    loss_per_epoch=0\n",
    "    for i in tqdm.tqdm(train_dataaaa):\n",
    "        data=Data(x=graph_data.x[i.subset],edge_index=i.edge_index,node_deg_norm=graph_data.node_deg_norm[i.edge_mask],\n",
    "                types_of_edges=graph_data.types_of_edges[i.edge_mask],types_message_scaling=graph_data.types_message_scaling[i.edge_mask],\n",
    "                label=i.label,disease_genes=i.disease_genes,protein_mask=graph_data.protein_mask[i.subset],batch=torch.zeros(graph_data.x[i.subset].shape[0],dtype=torch.long))\n",
    "        data=data.to(device)\n",
    "        model_out=model(data.x,data.edge_index,data.disease_genes,data.protein_mask,data.types_of_edges,data.node_deg_norm,data.types_message_scaling,data.batch)\n",
    "        if Focal_loss:\n",
    "            loss=focal_loss(model_out[data.protein_mask],data.label[data.protein_mask],gamma,alpha,epsilon)\n",
    "        else:\n",
    "            pos_weight=sum(data.label[data.protein_mask]==0)//sum(data.label[data.protein_mask]==1)\n",
    "            loss=F.binary_cross_entropy_with_logits(model_out[data.protein_mask],data.label[data.protein_mask].reshape(-1,1).to(dtype=torch.float32),\n",
    "                                                    pos_weight=torch.tensor(pos_weight))\n",
    "\n",
    "        l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "        loss = loss + L1_Reg * l1_norm\n",
    "        loss_per_epoch+=loss.item()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        del data,model_out\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    random.shuffle(train_dataaaa)\n",
    "    return loss_per_epoch/len(diseases)\n",
    "\n",
    "def test_list(model,graph_data,test_dataaaa,device,gamma,alpha,Focal_loss,threshold,epsilon):\n",
    "    diseases=list(set([i.name for i in test_dataaaa]))\n",
    "    model.eval()\n",
    "    loss_per_epoch=0\n",
    "    predictions_all=[]\n",
    "    labels_all=[]\n",
    "    probability_all=[]\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm.tqdm(test_dataaaa):\n",
    "            data=Data(x=graph_data.x[i.subset],edge_index=i.edge_index,node_deg_norm=graph_data.node_deg_norm[i.edge_mask],\n",
    "                    types_of_edges=graph_data.types_of_edges[i.edge_mask],types_message_scaling=graph_data.types_message_scaling[i.edge_mask],\n",
    "                    label=i.label,disease_genes=i.disease_genes,protein_mask=graph_data.protein_mask[i.subset],batch=torch.zeros(graph_data.x[i.subset].shape[0],dtype=torch.long))\n",
    "            data=data.to(device)\n",
    "            model_out=model(data.x,data.edge_index,data.disease_genes,data.protein_mask,data.types_of_edges,data.node_deg_norm,data.types_message_scaling,data.batch)\n",
    "            if Focal_loss:\n",
    "                loss=focal_loss(model_out[data.protein_mask],data.label[data.protein_mask],gamma,alpha,epsilon)\n",
    "            else:\n",
    "                pos_weight=sum(data.label[data.protein_mask]==0)//sum(data.label[data.protein_mask]==1)\n",
    "                loss=F.binary_cross_entropy_with_logits(model_out[data.protein_mask],data.label[data.protein_mask].reshape(-1,1).to(dtype=torch.float32),\n",
    "                                                        pos_weight=torch.tensor(pos_weight))\n",
    "\n",
    "            loss_per_epoch+=loss.item()\n",
    "            probability=F.sigmoid(model_out)\n",
    "            predictions=(probability>threshold).long()\n",
    "            probability_all.extend(probability[data.protein_mask].cpu())\n",
    "            predictions_all.extend(predictions[data.protein_mask].cpu())\n",
    "            labels_all.extend(data.label[data.protein_mask].cpu())\n",
    "            del data,model_out\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    return loss_per_epoch/len(diseases),np.array(labels_all),np.array(probability_all),np.array(predictions_all)\n",
    "\n",
    "def load_model_from_checkpoint(path,model,optimizer,scheduler,scaler,device):\n",
    "    model.to(device)\n",
    "    checkpoint = torch.load(path,weights_only=True,map_location=device) # initialize the model, scaler, optimizer, scheduler\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])   \n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "    new_epoch=checkpoint['epoch']+1\n",
    "    return model,optimizer,scheduler,scaler,new_epoch\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43b92e6-3562-410e-b769-97515c1db484",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /home/bionik/TP/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ba84ab-35af-4ebd-b5a1-611682e25a04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epsilon=0.1\n",
    "threshold=0.03\n",
    "num_folds = 5\n",
    "gamma=1.3\n",
    "alpha=0.95\n",
    "Focal_loss=True\n",
    "rels=5\n",
    "epochs=2\n",
    "L1_Reg=0.0001\n",
    "max_norm=2.0\n",
    "weight_decay=0.000075\n",
    "drop_out=0.35\n",
    "lr=0.000075\n",
    "input_layer=68\n",
    "conv_dim=26\n",
    "output_layer=1\n",
    "pos_enc=4\n",
    "heads=1\n",
    "feed_forward_transformer=32\n",
    "hidden_last_layer=16\n",
    "step_size=30\n",
    "gamma_scheduler=0.50\n",
    "train_from_checkpoint=True\n",
    "\n",
    "writer = SummaryWriter(log_dir=f'/home/bionik/TP/')\n",
    "writer.add_text('Parameters',str(f\" Gamma: {gamma} \\nAlpha: {alpha} \\nLearning_rate: {lr} \\nWeight_decay: {weight_decay} \\nMax_norm: {max_norm} \\n\\\n",
    "Dropout: {drop_out} \\nEpochs: {epochs} \\nFocal_loss: {Focal_loss} \\nthreshold: {threshold} \\ninput_dim: {input_layer} \\n\\\n",
    "output_layer: {output_layer} \\nheads: {heads} \\nfeed_forward_transformer: {feed_forward_transformer} \\n\\\n",
    "hidden_last_layer: {hidden_last_layer} \\nepsilon: {epsilon} \\nrelation: {rels} \\npos_enc: {pos_enc} \\nstepsize: {step_size} \\n gamma_scheduler: {gamma_scheduler}\"))\n",
    "writer.flush()\n",
    "\n",
    "device = torch.device('cuda')\n",
    "best_recall ,best_precision= float(0),float(0)\n",
    "scaler=GradScaler(device=device)\n",
    "model=diff_model_final_model(input_layer=input_layer,pos_enc=pos_enc,rels=rels,conv_dim=conv_dim,output_layer=output_layer,heads=heads,\n",
    "                  feed_forward_transformer=feed_forward_transformer,drop_out=drop_out,\n",
    "                  hidden_last_layer=hidden_last_layer)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma_scheduler)\n",
    "\n",
    "\n",
    "if train_from_checkpoint:\n",
    "    model,optimizer,scheduler,scaler,new_epoch=load_model_from_checkpoint('/home/bionik/TP/epoch_model_optimizer_scheduler.pth',model,optimizer,scheduler,scaler,device)\n",
    "    \n",
    "init=new_epoch if train_from_checkpoint else 1\n",
    "epochs=init+epochs\n",
    "\n",
    "model.to(device)\n",
    "for i in range(init,epochs):\n",
    "    print('Doing:',i)\n",
    "    train_loss=train_list(model,graph_data,train_data,device,gamma,alpha,Focal_loss,max_norm,epsilon,L1_Reg)\n",
    "    scheduler.step()\n",
    "    test_loss,label,prob,pred=test_list(model,graph_data,val_data,device,gamma,alpha,Focal_loss,threshold,epsilon)\n",
    "\n",
    "\n",
    "    print('Done with :',i)\n",
    "    f1_=f1_score(label,pred,zero_division=0)\n",
    "    f2_=fbeta_score(label,pred,beta=2,average='binary')\n",
    "    recall=recall_score(label,pred,zero_division=0)\n",
    "    accuracy=accuracy_score(label,pred)\n",
    "    precision=precision_score(label,pred,zero_division=0)\n",
    "    roc=roc_auc_score(label,prob)\n",
    "    avr_pr=average_precision_score(label,prob)\n",
    "\n",
    "\n",
    "    writer.add_pr_curve(f'fold_{1}/Precision_recall_curve',np.array(label),np.array(prob).flatten(),global_step=i)\n",
    "    writer.add_scalar(f'fold_{1}/Avg_Precision_recall', avr_pr, global_step=i)\n",
    "    writer.add_scalar(f'fold_{1}/Recall', recall, global_step=i)\n",
    "    writer.add_scalar(f'fold_{1}/ROC', roc, global_step=i)\n",
    "    writer.add_scalar(f'fold_{1}/Accuracy', accuracy, global_step=i)\n",
    "    writer.add_scalar(f'fold_{1}/Precision', precision, global_step=i)\n",
    "    writer.add_scalar(f'fold_{1}/F1 Score', f1_, global_step=i)\n",
    "    writer.add_scalar(f'fold_{1}/F2 Score', f2_, global_step=i)\n",
    "    writer.add_scalars(f'fold_{1}/Loss',{'train':train_loss,'test':test_loss},global_step=i)\n",
    "    print('Done with adding scalers')\n",
    "\n",
    "    val,idx=torch.topk(torch.tensor(prob).flatten(),k=2)\n",
    "    f1_=f1_score(label[idx],pred[idx],zero_division=0)\n",
    "    f2_=fbeta_score(label[idx],pred[idx],beta=2,average='binary')\n",
    "    recall=recall_score(label[idx],pred[idx],zero_division=0)\n",
    "    accuracy=accuracy_score(label[idx],pred[idx])\n",
    "    precision=precision_score(label[idx],pred[idx],zero_division=0)\n",
    "    roc=roc_auc_score(label[idx],prob[idx])\n",
    "    avr_pr=average_precision_score(label[idx],prob[idx])\n",
    "    writer.add_pr_curve(f'TopK_{1}/Precision_recall_curve',np.array(label),np.array(prob).flatten(),global_step=i)\n",
    "    writer.add_scalar(f'TopK_{1}/Avg_Precision_recall', avr_pr, global_step=i)\n",
    "    writer.add_scalar(f'TopK_{1}/Recall', recall, global_step=i)\n",
    "    writer.add_scalar(f'TopK_{1}/ROC', roc, global_step=i)\n",
    "    writer.add_scalar(f'TopK_{1}/Accuracy', accuracy, global_step=i)\n",
    "    writer.add_scalar(f'TopK_{1}/Precision', precision, global_step=i)\n",
    "    writer.add_scalar(f'TopK_{1}/F1 Score', f1_, global_step=i)\n",
    "    writer.add_scalar(f'TopK_{1}/F2 Score', f2_, global_step=i)\n",
    "    \n",
    "    torch.save({'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'scaler_state_dict': scaler.state_dict(),\n",
    "                'epoch':i}, '/home/bionik/TP/epoch_model_optimizer_scheduler.pth')\n",
    "\n",
    "    if (recall >= best_recall) & (precision >= best_precision):\n",
    "        best_recall = recall\n",
    "        best_precision = precision\n",
    "        torch.save({'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'scaler_state_dict': scaler.state_dict(),\n",
    "                    'epoch':i}, '/home/bionik/TP/Best_model_optimizer_scheduler.pth')\n",
    "\n",
    "    del label,prob,pred, train_loss,test_loss\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    for name,para in model.named_parameters():\n",
    "        writer.add_histogram(f\"fold_{1}/Param_{name}\",para,i)\n",
    "\n",
    "    if i==1:\n",
    "        data_=Data(x=graph_data.x[train_data[0].subset],edge_index=train_data[0].edge_index,node_deg_norm=graph_data.node_deg_norm[train_data[0].edge_mask],\n",
    "                types_of_edges=graph_data.types_of_edges[train_data[0].edge_mask],types_message_scaling=graph_data.types_message_scaling[train_data[0].edge_mask],\n",
    "                label=train_data[0].label,disease_genes=train_data[0].disease_genes,protein_mask=graph_data.protein_mask[train_data[0].subset],batch=torch.zeros(graph_data.x[train_data[0].subset].shape[0],dtype=torch.long))\n",
    "        data_=data_.to(device)\n",
    "        writer.add_text('Model Architecture', str(model))\n",
    "        writer.add_graph(model,(data_.x,data_.edge_index,data_.disease_genes,data_.protein_mask,\n",
    "                                data_.types_of_edges,data_.node_deg_norm,data_.types_message_scaling,data_.batch))\n",
    "        del data_\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    writer.flush()\n",
    "\n",
    "\n",
    "writer.flush()\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bebb04-6ea1-4c58-871d-a6dafb65957f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5737816-6903-491d-b72d-f707cea5f811",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datsci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
